{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ov_filep= \"datasets/housing/major_project/outcome_variable_train.json\"\n",
    "ov_reader= pd.read_json(ov_filep,lines=True)\n",
    "y_values= ov_reader[\"outcome_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48101"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading training data\n",
    "\n",
    "train_features_fp=\"datasets/housing/major_project/TrainingFeatures.json\"\n",
    "train_feature_reader= pd.read_json(train_features_fp,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding maximum values training vector\n",
    "\n",
    "max_values_list=[]\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "max_values_list.append(0)\n",
    "\n",
    "for i in range(0,len(train_feature_reader[\"text_length\"]),1):\n",
    "    if(train_feature_reader[\"text_length\"][i]>max_values_list[0]):\n",
    "        max_values_list[0]=train_feature_reader[\"text_length\"][i]\n",
    "        \n",
    "\n",
    "for i in range(0,len(train_feature_reader[\"char_count\"]),1):\n",
    "    if(train_feature_reader[\"char_count\"][i]>max_values_list[1]):\n",
    "        max_values_list[1]=train_feature_reader[\"char_count\"][i]\n",
    "\n",
    "        \n",
    "for i in range(0,len(train_feature_reader[\"word_count\"]),1):\n",
    "    if(train_feature_reader[\"word_count\"][i]>max_values_list[2]):\n",
    "        max_values_list[2]=train_feature_reader[\"word_count\"][i]\n",
    "\n",
    "        \n",
    "for i in range(0,len(train_feature_reader[\"unique_word_count\"]),1):\n",
    "    if(train_feature_reader[\"unique_word_count\"][i]>max_values_list[3]):\n",
    "        max_values_list[3]=train_feature_reader[\"unique_word_count\"][i]\n",
    "\n",
    "        \n",
    "for i in range(0,len(train_feature_reader[\"sent_count\"]),1):\n",
    "    if(train_feature_reader[\"sent_count\"][i]>max_values_list[4]):\n",
    "        max_values_list[4]=train_feature_reader[\"sent_count\"][i]\n",
    "\n",
    "        \n",
    "for i in range(0,len(train_feature_reader[\"ari\"]),1):\n",
    "    if(train_feature_reader[\"ari\"][i]>max_values_list[5]):\n",
    "        max_values_list[5]=train_feature_reader[\"ari\"][i]\n",
    "        \n",
    "        \n",
    "for i in range(0,len(train_feature_reader[\"stars\"]),1):\n",
    "    if(train_feature_reader[\"stars\"][i]>max_values_list[6]):\n",
    "        max_values_list[6]=train_feature_reader[\"stars\"][i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26938, 21641, 5168, 1205, 197, 354.1012630014859, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % done...\n",
      "20.0 % done...\n",
      "30.0 % done...\n",
      "40.0 % done...\n",
      "50.0 % done...\n",
      "60.0 % done...\n",
      "70.0 % done...\n",
      "80.0 % done...\n",
      "90.0 % done...\n",
      "100.0 % done...\n"
     ]
    }
   ],
   "source": [
    "# training the machine learning model\n",
    "# we will be using the gradient descent model(linear regression) as our dataset is quite big so direct matrix multiplication \n",
    "# approach for linear regression won't work out\n",
    "#don't run this code again\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "theetah=[0.241190573011,-3.35322291101,-2.73794582735,0.0637142409019,3.16175222839,1.1072712316,1.77751336044,0.394354951208]\n",
    "\n",
    "#intial values of the coefficients\n",
    "# for i in range(0,8,1):\n",
    "#     theetah.append(0)\n",
    "\n",
    "# alpha learning rate    \n",
    "learning_rate=0.01\n",
    "\n",
    "#outcome_variable_values\n",
    "ov_filep= \"datasets/housing/major_project/outcome_variable_train.json\"\n",
    "ov_reader= pd.read_json(ov_filep,lines=True)\n",
    "y_values= ov_reader[\"outcome_var\"]\n",
    "\n",
    "#filep for writing theetah\n",
    "theetah_fp= open(\"datasets/housing/linear_regression_theetah_values.txt\",'w')\n",
    "\n",
    "# first 10 iterations \n",
    "for it in range(0,700,1):\n",
    "    #list of hypothesis function values of size 48101\n",
    "    hypo_values=[]\n",
    "    counter=0\n",
    "    #most time consuming part\n",
    "    for j in range(0,48101,1):\n",
    "        value_summer=0\n",
    "        value_summer+=(1.0/1.0)*theetah[0]\n",
    "        value_summer+=(train_feature_reader[\"text_length\"][j]/max_values_list[0])*theetah[1]\n",
    "        value_summer+=(train_feature_reader[\"char_count\"][j]/max_values_list[1])*theetah[2]\n",
    "        value_summer+=(train_feature_reader[\"word_count\"][j]/max_values_list[2])*theetah[3]\n",
    "        value_summer+=(train_feature_reader[\"unique_word_count\"][j]/max_values_list[3])*theetah[4]\n",
    "        value_summer+=(train_feature_reader[\"sent_count\"][j]/max_values_list[4])*theetah[5]\n",
    "        value_summer+=(train_feature_reader[\"ari\"][j]/max_values_list[5])*theetah[6]\n",
    "        value_summer+=(train_feature_reader[\"stars\"][j]/max_values_list[6])*theetah[7]\n",
    "        hypo_values.append(value_summer)\n",
    "\n",
    "    for j in range(0,8,1):\n",
    "        local_sum=0\n",
    "        local_list=[]\n",
    "        if(j==1):\n",
    "            local_list=train_feature_reader[\"text_length\"]\n",
    "        elif(j==2):\n",
    "            local_list=train_feature_reader[\"char_count\"]\n",
    "        elif(j==3):\n",
    "            local_list=train_feature_reader[\"word_count\"]\n",
    "        elif(j==4):\n",
    "            local_list=train_feature_reader[\"unique_word_count\"]\n",
    "        elif(j==5):\n",
    "            local_list=train_feature_reader[\"sent_count\"]\n",
    "        elif(j==6):\n",
    "            local_list=train_feature_reader[\"ari\"]\n",
    "        elif(j==7):\n",
    "            local_list=train_feature_reader[\"stars\"]\n",
    "            \n",
    "        for k in range(0,48101,1):\n",
    "            if(j==0):\n",
    "                local_sum+= (hypo_values[k]-y_values[k])*(1)\n",
    "            else:\n",
    "                local_sum+= (hypo_values[k]-y_values[k])*(local_list[k])\n",
    "        \n",
    "        theetah[j]= theetah[j]-((learning_rate*local_sum)/48101)\n",
    "    \n",
    "    if(((it+1)%70)==0):\n",
    "        print(((it+1)/700)*100, \"% done...\") \n",
    "    \n",
    "    for l in range(0,8,1):\n",
    "        theetah_fp.write(str(theetah[l])+\",\")\n",
    "        \n",
    "    theetah_fp.write('\\n')\n",
    "     \n",
    "    \n",
    "theetah_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29866856985740642,\n",
       " -3.2936309072163219,\n",
       " -2.0975154748221323,\n",
       " -0.24840780813661245,\n",
       " 2.864413458669449,\n",
       " 1.2894638317549558,\n",
       " 1.3570455979916116,\n",
       " 0.37399572889757193]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coeff_list=[0.158824755788,-3.35572258962,-3.02970651977,0.0421422403508,3.62331802454,0.604746729851,2.42838593796,0.420410130217]\n",
    "\n",
    "# coeff_list=[0.241190573011,-3.35322291101,-2.73794582735,0.0637142409019,3.16175222839,1.1072712316,1.77751336044,0.394354951208]\n",
    "\n",
    "coeff_list=[0.29866856985740642,\n",
    " -3.2936309072163219,\n",
    " -2.0975154748221323,\n",
    " -0.24840780813661245,\n",
    " 2.864413458669449,\n",
    " 1.2894638317549558,\n",
    " 1.3570455979916116,\n",
    " 0.37399572889757193]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.7664309664 % for training dataset\n"
     ]
    }
   ],
   "source": [
    "#efficiency for training vector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_filep= pd.read_json(\"datasets/housing/major_project/TrainingFeatures.json\",lines=True)\n",
    "\n",
    "predicted_ov=[]\n",
    "\n",
    "for i in range(0,len(train_filep),1):\n",
    "    temp=0\n",
    "    temp+= coeff_list[0]\n",
    "    temp+= coeff_list[1]*(train_filep[\"text_length\"][i]/max_values_list[0])\n",
    "    temp+= coeff_list[2]*(train_filep[\"char_count\"][i]/max_values_list[1])\n",
    "    temp+= coeff_list[3]*(train_filep[\"word_count\"][i]/max_values_list[2])\n",
    "    temp+= coeff_list[4]*(train_filep[\"unique_word_count\"][i]/max_values_list[3])\n",
    "    temp+= coeff_list[5]*(train_filep[\"sent_count\"][i]/max_values_list[4])\n",
    "    temp+= coeff_list[6]*(train_filep[\"ari\"][i]/max_values_list[5])\n",
    "    temp+= coeff_list[7]*(train_filep[\"stars\"][i]/max_values_list[6])\n",
    "    predicted_ov.append(temp)    \n",
    "\n",
    "ov_train= pd.read_json(\"datasets/housing/major_project/outcome_variable_train.json\",lines=True)    \n",
    "\n",
    "avg=0\n",
    "for i in range(0,len(ov_train[\"outcome_var\"]),1):\n",
    "    avg+=ov_train[\"outcome_var\"][i]\n",
    "\n",
    "avg= avg/len(ov_train[\"outcome_var\"])\n",
    "\n",
    "num=0\n",
    "deno=0\n",
    "\n",
    "for i in range(0,48101,1):\n",
    "    deno+= (ov_train[\"outcome_var\"][i]-avg)**2\n",
    "    \n",
    "for i in range(0,48101,1):\n",
    "    num+= (ov_train[\"outcome_var\"][i]-predicted_ov[i])**2\n",
    "    \n",
    "efficiency = 1-(num/deno)\n",
    "\n",
    "print(efficiency*100 ,\"% for training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading testing data\n",
    "\n",
    "test_features_fp=\"datasets/housing/major_project/TestingFeatures.json\"\n",
    "test_feature_reader= pd.read_json(test_features_fp,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6223269407 % for testing data\n"
     ]
    }
   ],
   "source": [
    "#efficiency for testing vector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_filep= pd.read_json(\"datasets/housing/major_project/TestingFeatures.json\",lines=True)\n",
    "\n",
    "predicted_ov=[]\n",
    "\n",
    "for i in range(0,len(test_filep),1):\n",
    "    temp=0\n",
    "    temp+= coeff_list[0]\n",
    "    temp+= coeff_list[1]*(test_filep[\"text_length\"][i]/max_values_list[0])\n",
    "    temp+= coeff_list[2]*(test_filep[\"char_count\"][i]/max_values_list[1])\n",
    "    temp+= coeff_list[3]*(test_filep[\"word_count\"][i]/max_values_list[2])\n",
    "    temp+= coeff_list[4]*(test_filep[\"unique_word_count\"][i]/max_values_list[3])\n",
    "    temp+= coeff_list[5]*(test_filep[\"sent_count\"][i]/max_values_list[4])\n",
    "    temp+= coeff_list[6]*(test_filep[\"ari\"][i]/max_values_list[5])\n",
    "    temp+= coeff_list[7]*(test_filep[\"stars\"][i]/max_values_list[6])\n",
    "    predicted_ov.append(temp)    \n",
    "\n",
    "ov_test= pd.read_json(\"datasets/housing/major_project/outcome_variable_test.json\",lines=True)    \n",
    "\n",
    "avg=0\n",
    "for i in range(0,len(ov_test[\"outcome_var\"]),1):\n",
    "    avg+=ov_test[\"outcome_var\"][i]\n",
    "\n",
    "avg= avg/len(ov_test[\"outcome_var\"])\n",
    "\n",
    "num=0\n",
    "deno=0\n",
    "\n",
    "for i in range(0,12025,1):\n",
    "    deno+= (ov_test[\"outcome_var\"][i]-avg)**2\n",
    "    \n",
    "for i in range(0,12025,1):\n",
    "    num+= (ov_test[\"outcome_var\"][i]-predicted_ov[i])**2\n",
    "    \n",
    "efficiency =1- (num/deno)\n",
    "\n",
    "print(efficiency*100, \"% for testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
